Number of layers:3
Network size:[4, 4, 1]
Input count:1
Neurons:  layer 0
    neuron 0
    w:[-0.38096072]
    b:-1.61634592495
    a:<function act_sigmoid at 0x7f0e834a1de8>
    neuron 1
    w:[-3.59954327]
    b:-0.0843621932484
    a:<function act_tanh at 0x7f0e834a1ed8>
    neuron 2
    w:[1.99105106]
    b:-2.34974206057
    a:<function act_sigmoid at 0x7f0e834a1de8>
    neuron 3
    w:[-0.09164297]
    b:-1.28496878935
    a:<function act_sigmoid at 0x7f0e834a1de8>
  layer 1
    neuron 0
    w:[-2.48102599 -0.38354599  1.79448318 -0.93576048]
    b:0.0439409826221
    a:<function act_tanh at 0x7f0e834a1ed8>
    neuron 1
    w:[-1.24809024 -0.62223866  2.26298077  0.50881602]
    b:1.98818293971
    a:<function act_tanh at 0x7f0e834a1ed8>
    neuron 2
    w:[ 1.76190099 -1.81279283  0.34558556 -1.62357765]
    b:-2.77766357754
    a:<function act_sigmoid at 0x7f0e834a1de8>
    neuron 3
    w:[-1.09177186  1.6813324  -1.86738192 -1.65462862]
    b:-0.647567223984
    a:<function act_tanh at 0x7f0e834a1ed8>
  layer 2
    neuron 0
    w:[ 4.49362804  1.99293611  4.43732027 -0.95315001]
    b:-1.46079613154
    a:<function act_tanh at 0x7f0e834a1ed8>
Fitness of Network: -1.24750084414e-05